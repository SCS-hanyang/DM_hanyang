{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNt8p4Jr0wJYADgBAyJM5J1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SCS-hanyang/DM_hanyang/blob/main/DM_%EC%88%98%EC%97%85%EC%9E%90%EB%A3%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ema-pytorch"
      ],
      "metadata": {
        "id": "jiesOYnCAUBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Module, ModuleList\n",
        "from torch.utils.data import  DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.amp import autocast\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms as T, utils\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import math\n",
        "\n",
        "from ema_pytorch import EMA\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from pathlib import Path\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3GX0XQtIX2cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function"
      ],
      "metadata": {
        "id": "apgW6n0saIvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if callable(d) else d\n",
        "\n",
        "def identity(t, *args, **kwargs):\n",
        "    return t\n",
        "\n",
        "def cycle(dl):\n",
        "    while True:\n",
        "        for data in dl:\n",
        "            yield data\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "def cast_tuple(t, length = 1):\n",
        "    if isinstance(t, tuple):\n",
        "        return t\n",
        "    return ((t,) * length)\n",
        "\n",
        "def normalize_to_neg_one_to_one(img):\n",
        "    return img * 2 - 1\n",
        "\n",
        "def unnormalize_to_zero_to_one(t):\n",
        "    return (t + 1) * 0.5\n",
        "\n",
        "def divisible_by(numer, denom):\n",
        "    return (numer % denom) == 0\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "def view_image(imgs):\n",
        "    num_images = imgs.shape[0]\n",
        "\n",
        "    num_rows = 4\n",
        "    num_cols = 4\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img = imgs[i].cpu().detach().squeeze().numpy()\n",
        "        row = i // num_cols\n",
        "        col = i % num_cols\n",
        "\n",
        "        if num_rows == 1 and num_cols == 1:\n",
        "            ax = axes\n",
        "        elif num_rows == 1 or num_cols == 1:\n",
        "            ax = axes[max(row, col)]\n",
        "        else:\n",
        "            ax = axes[row, col]\n",
        "\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        ax.set_title(f'Image {i+1}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def view_process(imgs):\n",
        "    sample_indices = list(range(0, 1000 + 1, 50))\n",
        "\n",
        "    num_samples_to_plot = len(sample_indices)\n",
        "\n",
        "    num_cols = 5\n",
        "    num_rows = (num_samples_to_plot + num_cols - 1) // num_cols\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2.5, num_rows * 2.5))\n",
        "    axes = axes.flatten()\n",
        "    imgs = imgs.squeeze(0)\n",
        "\n",
        "    for i, data_idx in enumerate(sample_indices):\n",
        "\n",
        "        ax = axes[i]\n",
        "\n",
        "        if isinstance(imgs, torch.Tensor):\n",
        "\n",
        "            img = imgs[data_idx].cpu().detach().squeeze().numpy()\n",
        "        elif isinstance(imgs, np.ndarray):\n",
        "            img = imgs[data_idx].squeeze()\n",
        "\n",
        "\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        ax.set_title(f\"t_step : {data_idx + 1}\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    for j in range(num_samples_to_plot, num_rows * num_cols):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "_asOwXKLaYcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## schedule"
      ],
      "metadata": {
        "id": "75SrDNv-YmR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beta_schedule(timesteps):\n",
        "\n",
        "    scale = 1000 / timesteps\n",
        "    beta_start = scale * 0.0001\n",
        "    beta_end = scale * 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "8qpQnsJvYo2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "alpha cumprod schedule : $\\bar{\\alpha}_t = \\prod_{s=1}^{t} (1 - \\beta_s)$"
      ],
      "metadata": {
        "id": "H7c5nGIRgain"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def alpha_cumprod_schedule(betas):\n",
        "\n",
        "    alphas = 1. - betas\n",
        "    alpha_cumprod = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "    return alpha_cumprod\n",
        ""
      ],
      "metadata": {
        "id": "nyVnJd64Yyp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alphas_cumprod_prev(alphas_cumprod):\n",
        "\n",
        "    return F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n",
        "\n",
        "def sqrt_alphas_cumprod(alphas_cumprod):\n",
        "\n",
        "    return torch.sqrt(alphas_cumprod)\n",
        "\n",
        "def sqrt_one_minus_alphas_cumprod(alphas_cumprod):\n",
        "\n",
        "    return torch.sqrt(1. - alphas_cumprod)\n",
        "\n",
        "def sqrt_recip_alphas(betas):\n",
        "\n",
        "    alphas = 1. - betas\n",
        "    return torch.sqrt(1. / alphas)\n",
        "\n",
        "def sqrt_recip_one_minus_alphas_cumprod(alphas_cumprod):\n",
        "\n",
        "    return torch.sqrt(1. / (1 - alphas_cumprod))\n",
        "\n",
        "def posterior_variance(betas, alphas_cumprods_prev, alphas_cumprod):\n",
        "\n",
        "    return betas * (1. - alphas_cumprods_prev) / (1. - alphas_cumprod)\n"
      ],
      "metadata": {
        "id": "rzVuwOlSwtyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## forward process"
      ],
      "metadata": {
        "id": "6WNMMlXn0mZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0) = \\prod_{t=1}^{T} q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})$\n",
        "\n",
        "q_sample : $q(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})$"
      ],
      "metadata": {
        "id": "X3Dy1JrhhOa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_sample(x_start, t, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, noise = None):\n",
        "    noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "\n",
        "    sqrt_alpha_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alpha_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
        "\n",
        "    x_t = sqrt_alpha_cumprod_t * x_start + sqrt_one_minus_alpha_cumprod_t * noise\n",
        "\n",
        "    return x_t\n"
      ],
      "metadata": {
        "id": "KI1LDXpeZiFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## reverse process"
      ],
      "metadata": {
        "id": "y8LztzYW0rce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sqaui2aLhrKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "p sample : $p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_t, t), \\Sigma_\\theta(\\mathbf{x}_t, t))$\n",
        "\n",
        "model_mean $$\\frac{1}{\\sqrt{\\alpha_t}}\\left(\\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\right)$$\n",
        "\n",
        "posterior_variance\n",
        "$$\\tilde{\\beta}_t := \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t}\\beta_t$$"
      ],
      "metadata": {
        "id": "IbiBosd5h6vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def p_sample(\n",
        "    x,\n",
        "    t,\n",
        "    pretrained_model,\n",
        "    betas,\n",
        "    sqrt_recip_alphas,\n",
        "    sqrt_recip_one_minus_alphas_cumprod,\n",
        "    posterior_variance,\n",
        "    ):\n",
        "\n",
        "    noise = torch.randn_like(x_t)\n",
        "\n",
        "    sqrt_recip_alpha_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "    beta_t = extract(betas, t, x.shape)\n",
        "    sqrt_recip_one_minus_alpha_cumprod_t = extract(sqrt_recip_one_minus_alphas_cumprod, t, x.shape)\n",
        "\n",
        "    predicted_noise_epsilon = pretrained_model(x, t)\n",
        "\n",
        "    model_mean = sqrt_recip_alpha_t * (x_t - beta_t * sqrt_recip_one_minus_alpha_cumprod_t * predicted_noise_epsilon)\n",
        "\n",
        "    posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "\n",
        "    x_t = model_mean + posterior_variance_t * noise\n",
        "\n",
        "    return x_t"
      ],
      "metadata": {
        "id": "G7AzAfUau4x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def p_sample_loop(self, shape, return_all_timesteps = True):\n",
        "    batch, device = shape[0], self.device\n",
        "\n",
        "    img = torch.randn(shape, device = device)\n",
        "    imgs = [img]\n",
        "\n",
        "    x_start = None\n",
        "\n",
        "    for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
        "        img = self.p_sample(img, t)\n",
        "        imgs.append(img)\n",
        "\n",
        "    ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
        "\n",
        "    ret = self.unnormalize(ret)\n",
        "    return ret\n"
      ],
      "metadata": {
        "id": "4TEq6lLmvCit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss $$\\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}}\\left[\\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}, t)\\|^2\\right]$$"
      ],
      "metadata": {
        "id": "knxZ9EZ_jGRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def p_losses(x_start, t, noise = None):\n",
        "        b, c, h, w = x_start.shape\n",
        "\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "\n",
        "        x = q_sample(x_start = x_start, t = t, noise = noise)\n",
        "\n",
        "        model_out = model(x, t)\n",
        "        loss = F.mse_loss(model_out, noise, reduction = 'none')\n",
        "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
        "\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "p7ZF84shivKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNet"
      ],
      "metadata": {
        "id": "QRdhLWeBsY6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Upsample(dim, dim_out = None):\n",
        "    return nn.Sequential(\n",
        "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
        "        nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1)\n",
        "    )\n",
        "\n",
        "def Downsample(dim, dim_out = None):\n",
        "    return nn.Sequential(\n",
        "        Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2),\n",
        "        nn.Conv2d(dim * 4, default(dim_out, dim), 1)\n",
        "    )\n",
        "\n",
        "class RMSNorm(Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.scale = dim ** 0.5\n",
        "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(x, dim = 1) * self.g * self.scale"
      ],
      "metadata": {
        "id": "2qLDo9eu2Qv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(Module):\n",
        "    def __init__(self, dim, dim_out, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
        "        self.norm = RMSNorm(dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, scale_shift = None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.act(x)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "G5nja5-f2ekA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetBlock(Module):\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim = None, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim, dim_out * 2)\n",
        "        ) if exists(time_emb_dim) else None\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, dropout = dropout)\n",
        "        self.block2 = Block(dim_out, dim_out)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb = None):\n",
        "\n",
        "        scale_shift = None\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
        "            scale_shift = time_emb.chunk(2, dim = 1)\n",
        "\n",
        "        h = self.block1(x, scale_shift = scale_shift)\n",
        "\n",
        "        #h = self.block2(h)\n",
        "\n",
        "        return h + self.res_conv(x)"
      ],
      "metadata": {
        "id": "iveOaBoPKzge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        heads = 4,\n",
        "        dim_head = 32,\n",
        "        num_mem_kv = 4\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "\n",
        "        self.norm = RMSNorm(dim)\n",
        "\n",
        "        self.mem_kv = nn.Parameter(torch.randn(2, heads, dim_head, num_mem_kv))\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, dim, 1),\n",
        "            RMSNorm(dim)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n",
        "\n",
        "        mk, mv = map(lambda t: repeat(t, 'h c n -> b h c n', b = b), self.mem_kv)\n",
        "        k, v = map(partial(torch.cat, dim = -1), ((mk, k), (mv, v)))\n",
        "\n",
        "        q = q.softmax(dim = -2)\n",
        "        k = k.softmax(dim = -1)\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
        "\n",
        "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
        "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n",
        "        return self.to_out(out)\n"
      ],
      "metadata": {
        "id": "4ehQVZAu2lDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPosEmb(Module):\n",
        "    def __init__(self, dim, theta = 10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.theta = theta\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(self.theta) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb"
      ],
      "metadata": {
        "id": "tfzKYyrH2bLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_mults = (1, 2, 4),\n",
        "        channels = 1,\n",
        "        sinusoidal_pos_emb_theta = 10000,\n",
        "        dropout = 0.,\n",
        "        attn_dim_head = 32,\n",
        "        attn_heads = 4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "\n",
        "        init_dim = dim\n",
        "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding = 3)\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        # time embeddings\n",
        "\n",
        "        time_dim = dim * 4\n",
        "\n",
        "        sinu_pos_emb = SinusoidalPosEmb(dim, theta = sinusoidal_pos_emb_theta)\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            sinu_pos_emb,\n",
        "            nn.Linear(dim, time_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_dim, time_dim)\n",
        "        )\n",
        "\n",
        "        # attention\n",
        "\n",
        "        num_stages = len(dim_mults)\n",
        "        attn_heads = cast_tuple(attn_heads, num_stages)\n",
        "        attn_dim_head = cast_tuple(attn_dim_head, num_stages)\n",
        "\n",
        "        resnet_block = partial(ResnetBlock, time_emb_dim = time_dim, dropout = dropout)\n",
        "\n",
        "        # layers\n",
        "\n",
        "        self.downs = ModuleList([])\n",
        "        self.ups = ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, ((dim_in, dim_out), layer_attn_heads, layer_attn_dim_head) in enumerate(zip(in_out, attn_heads, attn_dim_head)):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(ModuleList([\n",
        "                resnet_block(dim_in, dim_in),\n",
        "                resnet_block(dim_in, dim_in),\n",
        "                AttentionBlock(dim_in, dim_head = layer_attn_dim_head, heads = layer_attn_heads) if ind == 1 else nn.Identity(),\n",
        "                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)\n",
        "            ]))\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = resnet_block(mid_dim, mid_dim)\n",
        "        self.mid_attn = AttentionBlock(mid_dim, heads = attn_heads[-1], dim_head = attn_dim_head[-1])\n",
        "        self.mid_block2 = resnet_block(mid_dim, mid_dim)\n",
        "\n",
        "        for ind, ((dim_in, dim_out), layer_attn_heads, layer_attn_dim_head) in enumerate(zip(*map(reversed, (in_out, attn_heads, attn_dim_head)))):\n",
        "            is_last = ind == (len(in_out) - 1)\n",
        "\n",
        "            self.ups.append(ModuleList([\n",
        "                resnet_block(dim_out + dim_in, dim_out),\n",
        "                resnet_block(dim_out + dim_in, dim_out),\n",
        "                AttentionBlock(dim_out, dim_head = layer_attn_dim_head, heads = layer_attn_heads) if ind == 1 else nn.Identity(),\n",
        "                Upsample(dim_out, dim_in) if not is_last else  nn.Conv2d(dim_out, dim_in, 3, padding = 1)\n",
        "            ]))\n",
        "\n",
        "        #if ind == 1 else nn.Identity(),\n",
        "        self.out_dim = channels\n",
        "\n",
        "        self.final_res_block = resnet_block(init_dim * 2, init_dim)\n",
        "        self.final_conv = nn.Conv2d(init_dim, self.out_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x, time, x_self_cond = None):\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "        r = x.clone()\n",
        "\n",
        "        t = self.time_mlp(time)\n",
        "\n",
        "        h = []\n",
        "\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            h.append(x)\n",
        "\n",
        "            #x = block2(x, t)\n",
        "            x = attn(x) + x\n",
        "            #h.append(x)\n",
        "\n",
        "            x = downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x) + x\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim = 1)\n",
        "            x = block1(x, t)\n",
        "\n",
        "            #x = torch.cat((x, h.pop()), dim = 1)\n",
        "            #x = block2(x, t)\n",
        "            x = attn(x) + x\n",
        "\n",
        "            x = upsample(x)\n",
        "\n",
        "\n",
        "        x = torch.cat((x, r), dim = 1)\n",
        "\n",
        "        x = self.final_res_block(x, t)\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "0XSrvv4cemOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diffusion Model"
      ],
      "metadata": {
        "id": "2YpEQ5Ew5-i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Diffusion(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        *,\n",
        "        image_size,\n",
        "        timesteps = 1000,\n",
        "        sampling_timesteps = 1000,\n",
        "        auto_normalize = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        self.channels = self.model.channels\n",
        "\n",
        "        self.image_size = image_size\n",
        "\n",
        "        betas = beta_schedule(timesteps)\n",
        "        betas = betas.to(device)\n",
        "\n",
        "        alphas = 1. - betas\n",
        "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n",
        "\n",
        "        timesteps, = betas.shape\n",
        "        self.num_timesteps = int(timesteps)\n",
        "\n",
        "        # sampling related parameters\n",
        "\n",
        "        self.sampling_timesteps = default(sampling_timesteps, timesteps)\n",
        "\n",
        "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
        "\n",
        "        register_buffer('betas', betas)\n",
        "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
        "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "\n",
        "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
        "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
        "        register_buffer('sqrt_recip_alphas', torch.sqrt(1. / alphas))\n",
        "        register_buffer('sqrt_recip_one_minus_alphas_cumprod', torch.sqrt(1. / (1 - alphas_cumprod)))\n",
        "\n",
        "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "        register_buffer('posterior_variance', posterior_variance)\n",
        "\n",
        "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n",
        "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.betas.device\n",
        "\n",
        "    def predict_start_from_noise(self, x_t, t, noise):\n",
        "        return (\n",
        "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
        "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
        "        )\n",
        "\n",
        "    def predict_noise_from_start(self, x_t, t, x0):\n",
        "        return (\n",
        "            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
        "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
        "        )\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def p_sample(self, x, time):\n",
        "\n",
        "        b, *_, device = *x.shape, self.device\n",
        "        t = torch.full((b,), time, device = device, dtype = torch.long)\n",
        "\n",
        "        noise = torch.randn_like(x) if time > 0 else 0\n",
        "\n",
        "        sqrt_recip_alpha_t = extract(self.sqrt_recip_alphas, t, x.shape)\n",
        "        beta_t = extract(self.betas, t, x.shape)\n",
        "        sqrt_recip_one_minus_alpha_cumprod_t = extract(self.sqrt_recip_one_minus_alphas_cumprod, t, x.shape)\n",
        "\n",
        "        predicted_noise_epsilon = self.model(x, t)\n",
        "\n",
        "        model_mean = sqrt_recip_alpha_t * (x - beta_t * sqrt_recip_one_minus_alpha_cumprod_t * predicted_noise_epsilon)\n",
        "\n",
        "        posterior_variance_t = extract(self.posterior_variance, t, x.shape)\n",
        "\n",
        "        pred_img = model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "        return pred_img\n",
        "\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def p_sample_loop(self, shape ,return_all_timesteps = False):\n",
        "        batch, device = shape[0], self.device\n",
        "\n",
        "        img = torch.randn(shape, device = device)\n",
        "        imgs = [img]\n",
        "\n",
        "        x_start = None\n",
        "\n",
        "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
        "            img = self.p_sample(img, t)\n",
        "            imgs.append(img)\n",
        "\n",
        "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
        "\n",
        "        ret = self.unnormalize(ret)\n",
        "        return ret\n",
        "\n",
        "\n",
        "    def sample(self, batch_size = 16 , return_all_timesteps = False):\n",
        "        (h, w), channels = (self.image_size,self.image_size), self.channels\n",
        "        sample_fn = self.p_sample_loop\n",
        "        return sample_fn((batch_size, channels, h, w), return_all_timesteps = return_all_timesteps)\n",
        "\n",
        "    @autocast('cuda', enabled = False)\n",
        "    def q_sample(self, x_start, t, noise = None):\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "\n",
        "        return (\n",
        "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
        "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
        "        )\n",
        "\n",
        "    def p_losses(self, x_start, t, noise = None):\n",
        "        b, c, h, w = x_start.shape\n",
        "\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "\n",
        "        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
        "\n",
        "        model_out = self.model(x, t)\n",
        "        loss = F.mse_loss(model_out, noise, reduction = 'none')\n",
        "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
        "\n",
        "        #loss = loss * extract(self.loss_weight, t, loss.shape)\n",
        "        return loss.mean()\n",
        "\n",
        "    def forward(self, img, *args, **kwargs):\n",
        "        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n",
        "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
        "\n",
        "        img = self.normalize(img)\n",
        "        return self.p_losses(img, t, *args, **kwargs)"
      ],
      "metadata": {
        "id": "JcmQJH3y59kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer\n"
      ],
      "metadata": {
        "id": "PWWMxZJOBV9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        diffusion_model,\n",
        "        *,\n",
        "        train_batch_size = 16,\n",
        "        gradient_accumulate_every = 1,\n",
        "        train_lr = 1e-4,\n",
        "        train_num_steps = 21000,\n",
        "        ema_update_every = 10,\n",
        "        ema_decay = 0.995,\n",
        "        adam_betas = (0.9, 0.99),\n",
        "        save_and_sample_every = 1000,\n",
        "        num_samples = 25,\n",
        "        results_folder = './results',\n",
        "        amp = True,\n",
        "        mixed_precision_type = 'fp16',\n",
        "        split_batches = True,\n",
        "        inception_block_idx = 2048,\n",
        "        max_grad_norm = 1.,\n",
        "        num_fid_samples = 50000,\n",
        "        save_best_and_latest_only = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # accelerator\n",
        "\n",
        "        self.accelerator = Accelerator(\n",
        "            split_batches = split_batches,\n",
        "            mixed_precision = mixed_precision_type if amp else 'no'\n",
        "        )\n",
        "\n",
        "        # model\n",
        "\n",
        "        self.model = diffusion_model\n",
        "        self.channels = diffusion_model.channels\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "        self.save_and_sample_every = save_and_sample_every\n",
        "\n",
        "        self.batch_size = train_batch_size\n",
        "        self.gradient_accumulate_every = gradient_accumulate_every\n",
        "\n",
        "        self.train_num_steps = train_num_steps\n",
        "        self.image_size = diffusion_model.image_size\n",
        "\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "        # dataset and dataloader\n",
        "\n",
        "        transform = T.Compose([\n",
        "            T.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        self.ds = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "\n",
        "        dl = DataLoader(self.ds, batch_size = train_batch_size, shuffle = True, pin_memory = True)\n",
        "\n",
        "        dl = self.accelerator.prepare(dl)\n",
        "        self.dl = cycle(dl)\n",
        "\n",
        "        # optimizer\n",
        "\n",
        "        self.opt = Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)\n",
        "\n",
        "        # for logging results in a folder periodically\n",
        "\n",
        "        if self.accelerator.is_main_process:\n",
        "            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every)\n",
        "            self.ema.to(self.device)\n",
        "\n",
        "        self.results_folder = Path(results_folder)\n",
        "        self.results_folder.mkdir(exist_ok = True)\n",
        "\n",
        "        # step counter state\n",
        "\n",
        "        self.step = 0\n",
        "\n",
        "        # prepare model, dataloader, optimizer with accelerator\n",
        "\n",
        "        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n",
        "\n",
        "        self.save_best_and_latest_only = save_best_and_latest_only\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.accelerator.device\n",
        "\n",
        "    def save(self, milestone):\n",
        "        if not self.accelerator.is_local_main_process:\n",
        "            return\n",
        "\n",
        "        data = {\n",
        "            'step': self.step,\n",
        "            'model': self.accelerator.get_state_dict(self.model),\n",
        "            'opt': self.opt.state_dict(),\n",
        "            'ema': self.ema.state_dict(),\n",
        "            'scaler': self.accelerator.scaler.state_dict() if exists(self.accelerator.scaler) else None,\n",
        "        }\n",
        "\n",
        "        torch.save(data, str(self.results_folder / f'model-{milestone}.pt'))\n",
        "\n",
        "    def load(self, milestone):\n",
        "        accelerator = self.accelerator\n",
        "        device = accelerator.device\n",
        "\n",
        "        data = torch.load(str(self.results_folder / f'model-{milestone}.pt'), map_location=device, weights_only=True)\n",
        "\n",
        "        model = self.accelerator.unwrap_model(self.model)\n",
        "        model.load_state_dict(data['model'])\n",
        "\n",
        "        self.step = data['step']\n",
        "        self.opt.load_state_dict(data['opt'])\n",
        "        if self.accelerator.is_main_process:\n",
        "            self.ema.load_state_dict(data[\"ema\"])\n",
        "\n",
        "        if 'version' in data:\n",
        "            print(f\"loading from version {data['version']}\")\n",
        "\n",
        "        if exists(self.accelerator.scaler) and exists(data['scaler']):\n",
        "            self.accelerator.scaler.load_state_dict(data['scaler'])\n",
        "\n",
        "    def train(self):\n",
        "        accelerator = self.accelerator\n",
        "        device = accelerator.device\n",
        "\n",
        "        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n",
        "\n",
        "            while self.step < self.train_num_steps:\n",
        "                self.model.train()\n",
        "\n",
        "                total_loss = 0.\n",
        "\n",
        "                for _ in range(self.gradient_accumulate_every):\n",
        "                    imgs, _ = next(self.dl)\n",
        "                    imgs = imgs.to(device)\n",
        "\n",
        "                    with self.accelerator.autocast():\n",
        "                        loss = self.model(imgs)\n",
        "                        loss = loss / self.gradient_accumulate_every\n",
        "                        total_loss += loss.item()\n",
        "\n",
        "                    self.accelerator.backward(loss)\n",
        "\n",
        "                pbar.set_description(f'loss: {total_loss:.4f}')\n",
        "\n",
        "                accelerator.wait_for_everyone()\n",
        "                accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "\n",
        "                self.opt.step()\n",
        "                self.opt.zero_grad()\n",
        "\n",
        "                accelerator.wait_for_everyone()\n",
        "\n",
        "                self.step += 1\n",
        "                if accelerator.is_main_process:\n",
        "                    self.ema.update()\n",
        "\n",
        "                    if self.step != 0 and divisible_by(self.step, self.save_and_sample_every):\n",
        "                        self.ema.ema_model.eval()\n",
        "\n",
        "                        with torch.inference_mode():\n",
        "                            milestone = self.step // self.save_and_sample_every\n",
        "                            batches = num_to_groups(self.num_samples, self.batch_size)\n",
        "                            all_images_list = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))\n",
        "\n",
        "                        all_images = torch.cat(all_images_list, dim = 0)\n",
        "\n",
        "                        utils.save_image(all_images, str(self.results_folder / f'sample-{milestone}.png'), nrow = int(math.sqrt(self.num_samples)))\n",
        "\n",
        "                        if self.save_best_and_latest_only:\n",
        "                            self.save(\"latest\")\n",
        "                        else:\n",
        "                            self.save(milestone)\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        accelerator.print('training complete')"
      ],
      "metadata": {
        "id": "jeNoNnEy_cHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "AF3jFjZZEMWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unet = Unet(dim = 32)\n",
        "model = Diffusion(model = unet, image_size=28)\n",
        "trainer = Trainer(diffusion_model = model, train_batch_size=128, save_and_sample_every = 1000,)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "lEDEiayACw3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet = Unet(dim = 32)\n",
        "model = Diffusion(model = unet, image_size=28)\n",
        "trainer = Trainer(diffusion_model = model, train_batch_size=64, save_and_sample_every = 1000,)\n",
        "trainer.load('10')\n",
        "imgs = model.sample()\n",
        "view_image(imgs)\n",
        "\n",
        "'''for x in range(1,22,5):\n",
        "    trainer.load(str(x))\n",
        "    imgs = model.sample()\n",
        "    view_image(imgs)\n",
        "'''"
      ],
      "metadata": {
        "id": "ncO_0Vp-Pkmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process = model.sample(batch_size = 1, return_all_timesteps=True)\n",
        "view_process(process)\n"
      ],
      "metadata": {
        "id": "K0G7Q7SeERdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nraCgDX3GVUn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}